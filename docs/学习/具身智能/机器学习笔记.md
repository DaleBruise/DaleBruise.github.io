# 机器学习笔记

---

此笔记记录了机器学习过程中的主要的知识点内容和理解。  

## 1.回归与逻辑回归

---

### 1.1 回归的定义与构建回归模型  

回归(regression)就是根据数据模式预测未来的数据走向。我们首先可以根据在图标中的样式观察两者是否有线性或者非线性关系，然后将模型适配至数据中，最后选择出最适配数据的模型。因此如何选择适当的模型是回归过程中的关键。相对书面一点，我们需要根据误差 $min\sum e$ 求回归系数 $\beta$ 。  

在python里面这一过程的迭代优化已经被封装起来，我们直接调用函数即可。  

### 1.2 R-squared  

实际的数据预测中，真实值、预测值和误差之间的关系如下：  

$$
Y(真实值) = Y(预测值) + E(误差)
$$

因此在 $y$ 处的总方差=回归模型的可解释方差+未解释方差

$$
SST = SSR + SSE
$$

对于一个好模型来讲 $SST$ 越接近1就是越接近真实值，由此可以定义一个评判一个模型好坏的标准$R-sqaured$：  

$$
R-squared = \frac{SSR}{SST}
$$

在工业界中 $R-sqaured$ 的最小值为0.8。  

### 1.3 多元回归与回归中的多重共线性  

在进行数据分析时，可能会有多个不同的数据都会对预测值有影响。比起使用单独自变量预测数据，我们可以使用多个自变量进行回归，这就是多元回归。  

简单的线性回归方程：
$$
y = \beta_{0} + \beta_{1}x
$$  

使用多个变量的线性多元回归方程: 

$$
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_2{2}...... + \beta_{k}x_{k}
$$

使用多元回归往往会带给我们更高的R-sqaured值，也就是意味着更正确的模型。但是其几个自变量也会存在一些问题，比如这些多元自变量中会存在线性相关的向量组。这样的关系（依赖关系），我们可以称之为回归中的多重共线性。  

我们为什么需要关注多重共线性？这是因为多重共线性会对模型带来信任挑战，即模型中的部分相关系数是不可靠的，这是为什么呢？  

假如三个自变量 $x_1, x_2, x_3$ 中， $x_1, x_3$ 有着 $x_1 = 2x_3$ 的线性关系，那么对于一个模型来讲，预测的结果并不存在唯一的解。也就是脱离了数学中方程的定义——一组自变量只对应一个因变量，如果这个定义被打破，那么就不存唯一的映射关系，我们也无法准确预测这个结果。  
若几个自变量之间存在一些线性关系，那么我们并不需要这么多组的数据也可以正确地预测，也就是对R-squared值不会有太多的影响。简而言之，我们要去发现回归中的多重共线性，并且除掉它。  

具体方法简而言之，我们可以求出自变量的个数个 $VIF$ 值（方差膨胀因子）。当 $VIF$ 值大于5时，就说明这个变量存在多重共线性。计算公式为：  

$$ 
VIF = \frac{1}{1 - R_{i}^2}
$$

其中 $R_i$ 是对应变量 $x_i$ 的R-sqaured值。 

### 1.4 回归中各变量的影响力  

我们计算了多重性，已经排除了非影响回归的因素。但这还不够，如果是一个变量，那么这个变量对回归的影响力是百分之百；但如果是多个变量，每一个变量对回归的影响不一定相同，所以我们要求出每一个变量对于回归的影响是多少，这是构建回归模型的必要步骤。  

使用t-test和P-value就可以检测出各变量对回归的影响程度。若P-value这个值大于0.05，说明该变量对回归的影响很小，计算方法已经集成到python的summary方法中，下面简单介绍一下：  

- 原假设 $H_0$ ：变量 $x_p$ 是没有影响力的变量( $\beta_0 = 0$ )
- 备择假设 $H_1$ ：变量 $x_p$ 是有影响力的变量( $\beta_0 \neq 0$ )
- 测试统计值：$t=\frac{\beta_p}{s(\beta_p)}$ 
- 如果 $t>t(/frac{\alpha}{2};n-k-1)$ 或者 $t<-t(/frac{\alpha}{2};n-k-1)$  并且设置 $\alpha$ 为5%，则拒绝原假设。

### 1.5 构建多元回归模型的步骤(总结)  

略

### 1.6 逻辑回归模型  

逻辑回归模型不同于线性回归模型，逻辑回归更倾向于模拟0或者1的情况，或者-1，0或者1的情况。是一种逻辑判断。但是数学中，没有绝对的逻辑回归连续函数，想要用连续函数来预测的话，我们可以使用指数函数来进行建模：  

$$
y=\frac{e^x}{1+e^x}
$$

对于逻辑回归模型中也有线性系数要求解：  

$$
y=\frac{e^{\beta_{0}+\beta_{1}x}}{1+e^{\beta_{0}+\beta_{1}x}}
$$

其中， $\beta_0,\beta_1$ 就是我们要求解的线性系数。 

### 1.7 逻辑回归线的准确率  

逻辑回归的计算方法与线性回归中的方法不太相同，但是思路上是大同小异的。我们根据预测的模型来根据原数据进行预测，然后将预测值与真实值进行比较。由于只有0和1的情况，所以只会存在一些以下四种情况： $[0,1], [0,0], [1,1], [1,0]$ 。  

这种矩阵叫做混淆矩阵(confusion matrix)，同时也可以根据此矩阵计算出一个准确率比值：  

$$
准确率 = \frac{正确分类的个数}{总个数}
$$

$$
准确率 = \frac{cm[0,0]+cm[1,1]}{cm[0,0]+cm[1,0]+cm[0,1]+cm[1,1]}
$$

### 1.8 多元逻辑回归的多重共线性和个体影响力  

同多元线性回归模型，此处略。  


## 2.决策树

---

### 2.1 什么是决策树？决策树的分类准则是什么？  

决策树类似一种逻辑分类，其基础是一种二叉树。每一层都根据一个元素分成两类，然后下一层在上一层的基础上就会再分，预测时根据这层分类标准进行遍历（先序遍历）。

### 2.2 决策树算法  

略

### 2.3 过拟合问题

略


## 3.模型选择和交叉验证  

---

### 




 
