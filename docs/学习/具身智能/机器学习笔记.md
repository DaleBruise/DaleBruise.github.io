# 机器学习笔记

---

此笔记记录了机器学习过程中的主要的知识点内容和理解。  

## 1 回归与逻辑回归

---

### 1.1 回归的定义与构建回归模型  

回归(regression)就是根据数据模式预测未来的数据走向。我们首先可以根据在图标中的样式观察两者是否有线性或者非线性关系，然后将模型适配至数据中，最后选择出最适配数据的模型。因此如何选择适当的模型是回归过程中的关键。相对书面一点，我们需要根据误差 $min\sum e$ 求回归系数 $\beta$ 。  

在python里面这一过程的迭代优化已经被封装起来，我们直接调用函数即可。  

### 1.2 R-squared  

实际的数据预测中，真实值、预测值和误差之间的关系如下：  

$$
Y(真实值) = Y(预测值) + E(误差)
$$

因此在 $y$ 处的总方差=回归模型的可解释方差+未解释方差

$$
SST = SSR + SSE
$$

对于一个好模型来讲 $SST$ 越接近1就是越接近真实值，由此可以定义一个评判一个模型好坏的标准$R-sqaured$：  

$$
R-squared = \frac{SSR}{SST}
$$

在工业界中 $R-sqaured$ 的最小值为0.8。  

### 1.3 多元回归与回归中的多重共线性  

在进行数据分析时，可能会有多个不同的数据都会对预测值有影响。比起使用单独自变量预测数据，我们可以使用多个自变量进行回归，这就是多元回归。  

简单的线性回归方程：
$$
y = \beta_{0} + \beta_{1}x
$$  

使用多个变量的线性多元回归方程: 

$$
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_2{2}...... + \beta_{k}x_{k}
$$

使用多元回归往往会带给我们更高的R-sqaured值，也就是意味着更正确的模型。但是其几个自变量也会存在一些问题，比如这些多元自变量中会存在线性相关的向量组。这样的关系（依赖关系），我们可以称之为回归中的多重共线性。  

我们为什么需要关注多重共线性？这是因为多重共线性会对模型带来信任挑战，即模型中的部分相关系数是不可靠的，这是为什么呢？  

假如三个自变量 $x_1, x_2, x_3$ 中， $x_1, x_3$ 有着 $x_1 = 2x_3$ 的线性关系，那么对于一个模型来讲，预测的结果并不存在唯一的解。也就是脱离了数学中方程的定义——一组自变量只对应一个因变量，如果这个定义被打破，那么就不存唯一的映射关系，我们也无法准确预测这个结果。  
若几个自变量之间存在一些线性关系，那么我们并不需要这么多组的数据也可以正确地预测，也就是对R-squared值不会有太多的影响。简而言之，我们要去发现回归中的多重共线性，并且除掉它。  

具体方法简而言之，我们可以求出自变量的个数个 $VIF$ 值（方差膨胀因子）。当 $VIF$ 值大于5时，就说明这个变量存在多重共线性。计算公式为：  

$$ 
VIF = \frac{1}{1 - R_{i}^2}
$$

其中 $R_i$ 是对应变量 $x_i$ 的R-sqaured值。 

### 1.4 回归中各变量的影响力  

我们计算了多重性，已经排除了非影响回归的因素。但这还不够，如果是一个变量，那么这个变量对回归的影响力是百分之百；但如果是多个变量，每一个变量对回归的影响不一定相同，所以我们要求出每一个变量对于回归的影响是多少，这是构建回归模型的必要步骤。  

使用t-test和P-value就可以检测出各变量对回归的影响程度。若P-value这个值大于0.05，说明该变量对回归的影响很小，计算方法已经集成到python的summary方法中，下面简单介绍一下：  

- 原假设 $H_0$ ：变量 $x_p$ 是没有影响力的变量( $\beta_0 = 0$ )
- 备择假设 $H_1$ ：变量 $x_p$ 是有影响力的变量( $\beta_0 \neq 0$ )
- 测试统计值：$t=\frac{\beta_p}{s(\beta_p)}$ 
- 如果 $t>t(\frac{\alpha}{2};n-k-1)$ 或者 $t<-t(\frac{\alpha}{2};n-k-1)$  并且设置 $\alpha$ 为5%，则拒绝原假设。

### 1.5 构建多元回归模型的步骤(总结)  

略

### 1.6 逻辑回归模型  

逻辑回归模型不同于线性回归模型，逻辑回归更倾向于模拟0或者1的情况，或者-1，0或者1的情况。是一种逻辑判断。但是数学中，没有绝对的逻辑回归连续函数，想要用连续函数来预测的话，我们可以使用指数函数来进行建模：  

$$
y=\frac{e^x}{1+e^x}
$$

对于逻辑回归模型中也有线性系数要求解：  

$$
y=\frac{e^{\beta_{0}+\beta_{1}x}}{1+e^{\beta_{0}+\beta_{1}x}}
$$

其中， $\beta_0,\beta_1$ 就是我们要求解的线性系数。 

### 1.7 逻辑回归线的准确率  

逻辑回归的计算方法与线性回归中的方法不太相同，但是思路上是大同小异的。我们根据预测的模型来根据原数据进行预测，然后将预测值与真实值进行比较。由于只有0和1的情况，所以只会存在一些以下四种情况： $[0,1], [0,0], [1,1], [1,0]$ 。  

这种矩阵叫做混淆矩阵(confusion matrix)，同时也可以根据此矩阵计算出一个准确率比值：  

$$
准确率 = \frac{正确分类的个数}{总个数}
$$

$$
准确率 = \frac{cm[0,0]+cm[1,1]}{cm[0,0]+cm[1,0]+cm[0,1]+cm[1,1]}
$$

### 1.8 多元逻辑回归的多重共线性和个体影响力  

同多元线性回归模型，此处略。  


## 2 决策树

---

### 2.1 什么是决策树？决策树的分类准则是什么？  

决策树类似一种逻辑分类，其基础是一种二叉树。每一层都根据一个元素分成两类，然后下一层在上一层的基础上就会再分，预测时根据这层分类标准进行遍历（先序遍历）。

### 2.2 决策树算法  

略

### 2.3 过拟合问题

略


## 3 模型选择和交叉验证  

---

首先，我们要谨慎地选择数据集，并且对原始的数据集要有一定的筛选和处理。然后我们就可以进型建模，建模过后不能过早地看到建模的结果很好就部署，而是要对模型有一定的选择过程。就是因为你选择的模型并不一定是最佳答案，或者可能会有更正确的答案，所以我们要对模型进行流程上的验证。  

同时，部署模型过后，也并不是什么都不做，因为总会有新的数据到来。新的数据会不会不适应这个“过去”的模型？也会是一个问题。所以我们要进行进一步的验证，也就是既要满足过去的数据集，也要满足新的数据集，让模型更加健壮和长时间正确。  

### 3.1 构建模型的步骤

构建模型的生命周期分为以下五个步骤：  
1. **定义模型并完成目标。**
	首先是要定义我们的任务是什么？完整的目标是什么？然后去分析数据的输入和输出的特征。  
2. **探索、验证及准备数据。**  
	- 对数据集的row和column进行分辨，查看有没有一些明显的缺失等问题
	- 分析每一个单row或者column，查看有没有奇异值
	- 根据上述的分析，筛选出干净有用的数据开始分析
	- 对一些文本数据转换成数字睡觉  
3. **构建模型。**  
	- 选择合适的模型
	- 查看数据集的类型是分类数据还是回归数据
	- 选择相同类型的三个及以上的模型进行尝试
	- 尝试不同的特征来训练模型  
4. **验证模型。**  
	- 使用R-sqaured或者准确率进行必要的模型验证
	- 在训练数据集和预测数据集上进行验证
	- 找出可能使模型失败的数据集进行验证  
5. **部署模型。**  
	……  

### 3.2 模型验证指标：回归  

回归模型中我们基本上会围绕标准差和标准房差进行验证，除了R-squared之外，还包含：**MAD**，**MAPE**，**RMSE**。  

1. 平均绝对离差**MAD**
	该指标的公式如下所示：  
	$$
	MAD = \sum_{i=1}^n {\frac{\left\vert y_i - \hat{y_i} \right\vert}{n}}
	$$
	公式中可以看出，**MAD**其实就是标准差的均值，当**MAD**越小时，就说明数据的偏差越小。但是**MAD**的值没有很好的范围标准，当数据偏大时，**MAD**的值也会很大，但是并不一定代表数据会偏差很多。
2. 平均绝对离差百分比**MAPE**
	该指标的公式如下所示：
	$$
	MAPE = \frac{100}{n}\sum_{i=1}^n {\frac{\left\vert y_i - \hat{y_i} \right\vert}{y_i}}
	$$
	**MAPE**从公式中，可以发现本质上是**MAD**的百分比形式。在使用这个标准的时候，就可以不太关注数据的规模。
3. 均方根误差**RMSE**
	该指标的公式：  
	$$
	RMSE = \sqrt{\sum_{i=1}^n {\frac{(y_i - \hat{y_i})^2}{n}}}
	$$
	该指标越小，说明数据的偏差度就越小。  


### 3.3 模型验证指标：分类  

对于逻辑回归模型，也就是分类任务时，我们需要使用混淆矩阵和准确率来对模型进行验证。    

但是我们可能会遇到一种情况：准确率非常高的时候，也就是接近99%。那么我们预测正确数据，也就是 $cm[0,0]$ 的概率会很高，但是由于正确的错误数据相对太少，所以我们对整的错误数据，也就是 $cm[1,1]$ 的概率会很低，这是数据量不够导致的。为了判断这种情况是否存在，我们需要使用两种指标：**灵敏度**和**特异度**来进行判断。  

1. **灵敏度**
	灵敏度是第一个分类的准确率，按照惯例就是 $cm[0,0]$ 的比例，我们也称正例类的比例：  
	$$
	\begin{aligned}
	灵敏度 & = \frac{cm[0,0]}{cm[0,0] + cm[0,1]}\\
	& = \frac{真正例(TP)}{真正例(TP) + 假反例(FN)}\\
	\end{aligned}
	$$
2. **特异度**
	反之，特异度是第二个分类的准确率，按照惯例是 $cm[1,1]$ 的比例，我们也称正反例的比例：  
	$$
	\begin{aligned}
	特异度 & = \frac{cm[1,1]}{cm[1,1] + cm[1,0]}\\
	& = \frac{真反例(TN)}{真反例(TN) + 假正例(FP)}\\
	\end{aligned}
	$$  

一般这两种指标，不是只看其中一个。一般情况下，我们倾向于较高的灵敏度，较低的特异度，但是特异度不能像上述情况一样，过于低。所以我们需要另外的指标来使模型的灵敏度和特异度达到较为平衡的水平，也就是**ROC**和**AUC**。  

1. **ROC**受试者工作特征
	ROC是一种曲线，横坐标是特异度，纵坐标是灵敏度。二者往往会呈现一种对数函数曲线形式。我们通常会选择拐点的位置作为最终模型的灵敏度和特异度。
2. **AUC**
	AUC是ROC曲线的对角线。ROC曲线离AUC线越远，就代表模型越好。  

另外还有一种指标叫做**F1 Score**，F1 Score是灵敏度和特异度的一种拓展。

### 3.4 权衡偏差与方差  

模型也会有过拟合问题和欠拟合问题，拟合过度会导致未来数据的不精确，拟合欠缺会导致训练数据和未来数据与模型不匹配。因此一个好的模型不仅不能拟合过度，也要让拟合不那么欠缺。衡量这两个问题的指标为**方差**和**偏差**。  

模型中的广义误差可以分为三个部分，**不可约误差**，**偏差**和**方差**。  

**不可约误差**就是数据中总会存在一些无法减少的固有误差，就是无论你拟合出多么好的模型，都会有误差，不可能所有的数据都是100%0误差。**偏差**是由欠拟合造成的，**方差**是由过拟合造成的。总方差的公式如下：  

$$
总方差 = 不可约误差 + 偏差^2 + 方差
$$

**偏差**和**方差**往往是反方向增长的，当**偏差**减少时，可能**方差**会很大；**方差**很小时，**偏差**会很大。所以我们找到一个平衡点。  

### 3.5 交叉验证  

交叉验证是指在训练数据上构建模型，并且在测试数据上验证模型。测试数据和训练模型都是数据集中分散提取的两部分数据。 

目前有一种经典的**K-折交叉验证**，通常K是一个代表比例的一个数字。若K=10，则代表将整体数据集分成十份，即每一份占整体数据集的10%，这是第一步。  

其次，我们需要构建K个模型。首先我们对数据集的每一个字集编序号，我们可以将第一个字集作为验证子集，然后剩余的字集作为训练子集。后面的模型都是如此，第二个模型就是将第二个子集作为验证子集，剩余子集作为训练子集。重复这个步骤，我们就可以得到K个模型。  

最后，我们要将这几个模型都计算准确率，用平均准确率作为整体一次的建模过程的准确率。我们每一次调整参数都要看这个准确率。所以这个方法的时间复杂度和操作复杂度都很高，我们可以使用别的方法——**训练集-验证集-留出交叉验证方法**。  

差别在于，留出数据集是不用于前面的交叉验证过程和调整过程，而是用于最后的模型测试。所以在一定的方面能够平衡**K-折交叉验证**的高时间复杂度的问题。  

## 4 聚类分析  

---  

机器学习算法通常有两种学习方式，**无监督学习**和**有监督学习**。当我们的训练数据是有标签的，就可以使用**有监督学习**，即我们的学习是有正确答案给学习模型的；反之，则需要使用**无监督学习**，模型会通过不断的学习和调整，来尽可能地使学习的结果为正确的结果。  

### 4.1 无监督学习  

无监督学习主要有两种算法，一个是计算线性回归的**主成分分析**方法，另一个是计算逻辑回归的**聚类分析**方法。聚类分析是一种无监督的学习方法，它面临的挑战就是如何在没有“正确答案”的情况下，将数据分成正确的类别。  

其主要的思想跟决策树不同的点在于决策树是有“正确答案”的，即标签，但是聚类分析是不需要有标签的。突破点就在于虽然数据的分类没有“正确答案”，但是数据本身是具有一定的特征的，我们可以通过数据本身的特点来进行大致分类，这也是我们如何在实际的生活中进行分类的方法。例如你在面对一堆积木的情况下，如何把他们进行分类呢？无非是通过积木本身的一系列特征来分类：位置、体积、形状或者颜色等等。他们每一个积木很多不同的特征，运用哲学中的语言来讲，事物本身具有共性和个性，个性融入共性中，共性也脱离不开个性。  

那么具体如何实现这个思想呢？学术上，聚类分析一般将整体划分为几个子集，这几个子集会有一定的**相似性**和**差异性**（也就是我们上面讲到的共性和个性）。下面来介绍如何进行分别**相似性**和**差异性**：    

- 首先，假设要分类的集合的每个对象有两个特点。我们可以将这个特点数据化，然后使用二维坐标轴表示。这也就表示我们将这个对象使用两个维度进行分类。  
- 然后我们就会发现，部分对象会一个堆一个堆地出现在坐标轴上。从表象上来看，我们可以认为聚在一起的一个堆就是一类。  
- 至于如何判定一个堆的边界，我们就需要使用**欧几里得距离**来判断。在我们的这个例子中，就是两个点之间的距离。计算每个点（假设有N个点）之间的距离（无方向的距离），我们就可以得到一个对称的 $N \times N$ 距离矩阵 A。点 $A[i][j]$ 代表第 $i$ 个点到第 $j$ 个点之间的距离。  
- 我们可以通过这个距离矩阵来辅助我们设立**相似性**和**差异性**之间的界限。  

### 4.2 K-means聚类

帮助我们判断**相似性**和**差异性**之间的工具——距离矩阵完成了，但是我们该如何确定这个界限呢？  

我们可以设定一个类别的数目 $K=3$ ，规定他一共有3个类别。由于他这个分类是可能出现在任意区域的，而且我们后期可以通过迭代来调整和优化，所以最开始可以先随机设置3个点，作为3个类别的初始中心点。然后以一个初始的距离来进行对所有的除中心店以外的点进行分类。  

这样我们虽然得到了初步的三个类别，但这三个类别会很粗糙且不合理。不合理的点在于，有些点到这些中心点的距离会很大或者很小。我们要做的就是让这个距离总和或者偏差值做到最小，那么我们就可以不断地更新中心点的位置，让每个簇中的点到这个簇的中心点的距离偏差值最小。有点类似于**哈夫曼树的WPL**这个概念。这个就是**K-means聚类算法**，其中的K就是K个簇。  

**K-means聚类算法**具体是根据每次迭代簇的平均中心点来进行更新中心点的。也就是每个簇中所有点的平均值作为中心点，然后再次计算距离矩阵，更新簇的范围。然后再次重复这个步骤，最后这个中心点基本上不会有变动，就得到了最终的分类结果。  

### 4.3 聚类簇数量的选取  

**K-means聚类算法**的关键在于如何选择合适的簇的数量。我们一般使用“肘部法则（Elbow Method）”。  

我们可以选取不同的聚类簇来实现不同的分类结果，每一次分类结果都会有一个距离平方和，是簇中每个样本点的距离平方和：  

$$
Inertia = \sum_{i=1}^{n_1} {(x_i - {\mu}_1)^2} + \sum_{i=1}^{n_2} {(x_i - {\mu}_2)^2} + \cdots + \sum_{i=1}^{n_k} {(x_i - {\mu}_k)^2}
$$

其中，$\mu_1,\mu_2,\mu_k$ 就是每一次迭代的聚类中心。然后我们将这个Inertia值放在坐标轴上，然后我们选择的区域就是函数下降不快不慢的位置，也就是“肘部”区域。

## 5 随机森林和Boosting  

---  

之前的算法都是很优秀的算法，但是其准确性和鲁棒性不够强。我们如果想要追求更高的模型准确性，那么可以选择**黑箱方法**。包括**随机森林**、**Boosting**和**人工神经网络**等等，这些算法可以识别数据中的复杂模式，同时也会要求更高的终端计算性能。  

### 5.1 集成模型  

我们想要得到一个事物的评价，光靠一个可靠的、智慧的人的建议可能不太够。所以我们可以参考很多较不那么可靠的、不那么智慧的人的建议作为一个比较系统的建议，从而使我们的评价体系更全面和可靠。这就是**群体智慧**，也是**集成模型**的思想。  

**集成模型**就是我们可以建立很多个可靠模型，来得到一系列的结果，最后将这些结果总结和概括，得到一个最终的结果，这个结果往往会更加可靠和正确。这是由于，如果单个模型的正确率是在90%，那么10%的概率会犯错。但是如果是很多个正确率为80%的模型，那么至少有一半模型正确的概率就是：  

$$
\begin{aligned}
& 使用标准正态分布近似二项分布\\
& np = 100 * 0.8 = 80 > 5 \\
& \sigma^2 = np(1-p) = 100 * 0.8 * 0.2 = 16 \\
& z = \frac{49.5 - 80}{\sigma} = -7.625 \\
& z 远远小于 3，所以基本上概率是接近于 1 的。
\end{aligned}
$$

综上所述，**集成模型**是很有优势的。其中，**随机森林**就应用了**集成模型**的思想。在**Bagging**算法（是一种自动耦合和汇合投票结果的算法）的基础上，应用了决策树算法，并且在基础**Bagging**算法的基础上，进行了一定的改进。  

### 5.2 随机森林  

**随机森林**算法的步骤：  

- 使用K次自助采样选取模型的训练样本，K的大小越大越好
- 基于每一个训练样本，构建决策树模型。构建模型时不考虑所有的样本特征  
- 获取投票数量最高的类别，将该类别作为最终的预测结果

**随机森林**的随机性来源于，我们每次构建决策树时随机选择的样本特征。这样每次决策树构建时随特征（变量）的不同，也就带来的决策树模型的不同。  

在构建决策树时选择样本的特征数量 $p$ 称为**随机森林**中的**超参数**。一般 $p = \sqrt(t)$ ， $t$ 是所有特征（变量）数目。  

### 5.3 Boosting算法  

**Boosting**与**随机森林**算法不太相同，其是在**Bagging**算法的基础上，对每一个模型加上了一些权重，最后再将所有模型合并成一个模型。有两种比较经典的**Boosting**算法：**AdaBoosting**和**Gradient Boosting**。  

### 5.4 AdaBoosting  

**AdaBoosting**并不是类似**随机森林**一样，并行构建一系列模型，而是通过构建模型进行迭代。对每一个模型都有一定的打分系统。所以相比较其他算法，多了一个误差计算和加权样本模块。这个模块，会对于进行错误分类的记录进行更加高的权重，迭代到下一次的建模过程中。最后，误差减到最小，或者达到最大迭代限制后，算法会结束迭代过程。  

可以看出，在这个过程中，比较关键的过程就在于对错误预测的模型的权重计算和误差计算过程。具体的计算步骤如下：  

-  $N$ 表示训练集中的总记录数。每个样本的权重 $w_i$ ， $i$ 代表迭代次数，首先默认 $w_1 = 1/N$
- 构建模型 $M$ ，误差因子和准确率因子计算如下：

$$
\begin{aligned}
& 误差因子e_M = \frac{\sum_{i=1}^{N}{w_iI}}{\sum_{i=1}^N{w_i}} \\
& 准确率因子 \alpha_M = log\left(\frac{1-e_M}{e_M}\right) \\
\end{aligned}
$$

- 更新后的权重为：$w_{i+1} = w_ie^{\alpha_Ml}$ 。然后将权重进行标准化，让权重和为1。  

### 5.5 Gradient Boosting  

思想上同**AdaBoosting**，类似于线性回归中的梯度下降。  

## 6 人工神经网络  

---  

### 6.1 逻辑回归的网路图
 
